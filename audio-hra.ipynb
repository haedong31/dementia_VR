{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65447a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from clearml import Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb5d4a8",
   "metadata": {},
   "source": [
    "## ClearML agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c03453f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=0d5ed1cda5744de2b08e806f42d54bc3\n",
      "2022-10-13 13:19:48,524 - clearml.Task - INFO - No repository found, storing script code instead\n",
      "ClearML results page: https://app.community.clear.ml/projects/f99841227bad4da5b63d3ca4147ddaf0/experiments/0d5ed1cda5744de2b08e806f42d54bc3/output/log\n",
      "2022-10-13 13:19:49,063 - clearml.Task - WARNING - Skipping parameter: General/save_dir[WindowsPath], only builtin types are supported (NoneType, dict, list, tuple, str, int, float)\n",
      "{'num_of_epochs': 10, 'batch_size': 8, 'drop_out': 0.25, 'lr': 5e-05, 'save_dir': WindowsPath('results/20210520-peudo-eyetracking-cnn')}\n",
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    }
   ],
   "source": [
    "task = Task.init(project_name='VR Mental Health Clinic', \n",
    "    task_name='Audio IFS Plots - Pretrained ResNet')\n",
    "config_dict = {'num_of_epochs': 10, 'batch_size': 8, 'drop_out': 0.25, 'lr': 5e-5,\n",
    "               'save_dir': Path('./results/20210520-peudo-eyetracking-cnn')}\n",
    "config_dict = task.connect(config_dict)\n",
    "print(config_dict)\n",
    "\n",
    "data_dir = Path('./data/audio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc2004",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1974ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioIFSDataSet(Dataset):\n",
    "    def __init__(self,meta_df_path,data_dir):\n",
    "        self.paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "             transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "        \n",
    "        meta_df = pd.read_csv(meta_df_path)\n",
    "        num_id = 8\n",
    "        for _, row in meta_df.iterrows():\n",
    "            f = row['file']\n",
    "            l = row['label']\n",
    "            self.paths.append(data_dir/(f+'.png'))\n",
    "            self.labels.append(l)\n",
    "            \n",
    "            for i in range(num_id):\n",
    "                self.paths.append(data_dir/(f+'_'+str(i+1)+'.png'))\n",
    "                self.labels.append(l)\n",
    "                \n",
    "                for j in range(num_id):\n",
    "                    self.paths.append(data_dir/(f+'_'+str(i+1)+str(j+1)+'.png'))\n",
    "                    self.labels.append(l)\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        ifs_img = Image.open(self.paths[idx])\n",
    "        ifs_img = ifs_img.convert('RGB')\n",
    "        ifs_img = self.transform(ifs_img)\n",
    "        \n",
    "        return ifs_img, self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8834e7",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edccf67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('./data/audio')\n",
    "\n",
    "# data sets\n",
    "train_ds = AudioIFSDataSet(data_dir/'meta_train.csv', data_dir) \n",
    "valid_ds = AudioIFSDataSet(data_dir/'meta_valid.csv', data_dir)\n",
    "test_ds = AudioIFSDataSet(data_dir/'meta_test.csv', data_dir)\n",
    "\n",
    "# data loaders\n",
    "train_dl = DataLoader(train_ds, batch_size=config_dict.get('batch_size'))\n",
    "valid_dl = DataLoader(valid_ds, batch_size=config_dict.get('batch_size'))\n",
    "test_dl = DataLoader(test_ds, batch_size=config_dict.get('batch_size'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af90e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained ResNet\n",
    "model = models.resnet34(pretrained=True) # try from the empty model? put this option in the configuratin dictionary?\n",
    "model.fc = nn.Sequential(nn.Dropout(p=config_dict.get('drop_out')),\n",
    "                         nn.Linear(512,1),\n",
    "                         nn.Sigmoid())\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=config_dict.get('lr'))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=config_dict.get('num_epochs')//4, gamma=0.5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# device (GPU) setting\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "41490c706c2f5d934508b9157cd4aeb40558a7e0fa8a6061ce60f5cf39ecf8e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
